{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from typing import Optional, Callable, List, Union\n",
    "import os\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from omegaconf import OmegaConf\n",
    "import logging\n",
    "import multiprocessing\n",
    "from dotenv import load_dotenv\n",
    "import yaml\n",
    "from functools import partial\n",
    "import time\n",
    "from datetime import datetime\n",
    "import lm_eval\n",
    "from lm_eval.models.huggingface import HFLM\n",
    "import numpy as np\n",
    "\n",
    "# HF imports\n",
    "from datasets import load_dataset, Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, LoraConfig, get_peft_model\n",
    "\n",
    "from wmdp.rmu.utils import get_data, get_params, forward_with_cache\n",
    "from latent_at.lat_methods import (\n",
    "    projected_gradient_descent\n",
    ")\n",
    "from latent_at.utils import (\n",
    "    get_minibatch,\n",
    "    zero_nan_grads,\n",
    ")\n",
    "from latent_at.laa import (\n",
    "    CustomHook,\n",
    "    clear_hooks,\n",
    ")\n",
    "from latent_at.lat_helpers import (\n",
    "    do_defense_step,\n",
    ")\n",
    "from latent_at.lat_datasets import WMDPLATDataCollator, WMDPLATTargetedDataCollator\n",
    "from tasks.harmbench.FastHarmBenchEvals import run_general_evals\n",
    "from tasks.general_capabilities.multiple_choice_tasks import WMDPTask, MMLUTask\n",
    "from tasks.wmdp.WMDP_MCTask import WMDP_MCTask\n",
    "\n",
    "load_dotenv()\n",
    "hf_api_key = os.getenv(\"HF_API_KEY\")\n",
    "\n",
    "multiprocessing.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "\n",
    "model_dict = {\n",
    "    \"LLAMA_7B\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"LLAMA_34B\": \"meta-llama/Llama-2-34b-chat-hf\",\n",
    "    \"ZEPHYR_7B\": \"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    \"ZEPHYR_34B\": \"HuggingFaceH4/zephyr-34b-beta\",\n",
    "}\n",
    "\n",
    "\n",
    "class RunLAT:\n",
    "    def __init__(\n",
    "        self, \n",
    "        config: OmegaConf, \n",
    "        cuda_id: int,\n",
    "        sweep_params: Optional[list[str]],\n",
    "        init_callback: Optional[Callable] = None,\n",
    "        post_def_callback: Optional[Callable] = None,\n",
    "        post_adv_callback: Optional[Callable] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cuda_id: Each Run instance associated with a single GPU. Allows sweeps to call multiple Run instances on multiple GPUs.\n",
    "            sweep_params: Any parameters that are changed by the sweep.\n",
    "            init_callback: Typically, an evaluation function.\n",
    "            post_def_callback: \"\".\n",
    "            post_adv_callback: \"\".\n",
    "        \"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(f\"cuda:{cuda_id}\")\n",
    "        elif torch.backends.mps.is_available():\n",
    "            self.device = torch.device(\"mps\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "        \n",
    "        self.config = config\n",
    "        self.rmu_config = config.rmu_config\n",
    "\n",
    "        run = self._set_logger()\n",
    "        if sweep_params: # For wandb sweeps: update with wandb values\n",
    "            config = update_with_wandb_config(config, sweep_params)\n",
    "        self.run_name = f\"pgdlayers-{config.pgd_config.pgd_layers}_epsilon-{config.pgd_config.epsilon}_deflr-{config.def_config.outer_learning_rate}_pgdlr-{config.pgd_config.inner_learning_rate}_defsft-{config.def_config.def_loss_coefs.sft:.2f}_deftowards-{config.def_config.def_loss_coefs.toward:.2f}_defaway-{config.def_config.def_loss_coefs.away:.2f}_pgdtowards-{config.pgd_config.adv_loss_coefs.toward:.2f}_pgdaway-{config.pgd_config.adv_loss_coefs.away:.2f}_untargeted-{config.use_untargeted}_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "        run.name = self.run_name\n",
    "\n",
    "        # Model\n",
    "        self.model, self.tokenizer = self._load_model(hf_api_key)\n",
    "\n",
    "        ## Dataloaders\n",
    "        if config.use_untargeted: # Untargeted LAT, use unlearn for defence and adversary\n",
    "            forget_list = self._load_untargeted_wmdp_data()\n",
    "            self.bio_dataloader, self.cyber_dataloader = self._make_untargeted_wmdp_dataloaders(forget_list, self.tokenizer, sft=False)\n",
    "        else: # Use WMDP retain and unlearn - note logic is *totally different* to above\n",
    "            assert config.forget_corpora == 'bio-forget-corpus,cyber-forget-corpus'\n",
    "            bio_dataset, cyber_dataset = self._load_targeted_wmdp_data()\n",
    "            self.bio_dataloader, self.cyber_dataloader = self._make_targeted_wmdp_dataloaders(bio_dataset, cyber_dataset, self.tokenizer)\n",
    "        print(f\"Dataloader length {len(self.bio_dataloader)} {len(self.cyber_dataloader)}\")\n",
    "        self.bio_dataloader, self.cyber_dataloader = itertools.cycle(self.bio_dataloader), itertools.cycle(self.cyber_dataloader)\n",
    "        # SFT dataloader\n",
    "        if config.def_config.def_loss_coefs.sft > 0:\n",
    "            sft_dataset: list[str] = self._load_sft_dataset()\n",
    "            self.sft_dataloader = self._make_untargeted_wmdp_dataloaders(sft_dataset, self.tokenizer, sft=True)\n",
    "            self.sft_dataloader = itertools.cycle(self.sft_dataloader)\n",
    "        else:\n",
    "            assert config.def_config.def_loss_coefs[\"sft\"] == 0\n",
    "            self.sft_dataloader = None\n",
    "\n",
    "        # Callbacks\n",
    "        self.init_callback = init_callback\n",
    "        self.post_adv_callback = post_adv_callback\n",
    "        self.post_def_callback = post_def_callback\n",
    "        \n",
    "        if config.only_train_lora is None:\n",
    "            self.only_train_lora = isinstance(self.model, PeftModel)\n",
    "        else:\n",
    "            self.only_train_lora = config.only_train_lora\n",
    "\n",
    "        # Optimisers\n",
    "        # PGD optim initialized in projected_gradient_descent function\n",
    "        def_lr = self.config.def_config.outer_learning_rate\n",
    "        self.def_optim = torch.optim.Adam(self.model.parameters(), lr=def_lr)\n",
    "        if config.def_config.use_cosine_schedule:\n",
    "            self.schedule = self._make_lr_schedule(base_lr=def_lr, final_lr=def_lr/5, num_epochs=1, iter_per_epoch=config.num_epochs)\n",
    "\n",
    "        self.model_layers = self.config.model_layers if self.config.model_layers is not None else range(self.model.config.num_hidden_layers)\n",
    "\n",
    "\n",
    "    def _make_lr_schedule(self, base_lr, final_lr, num_epochs, iter_per_epoch):\n",
    "        decay_iter = iter_per_epoch * num_epochs\n",
    "        return final_lr+0.5*(base_lr-final_lr)*(1+np.cos(np.pi*np.arange(decay_iter)/decay_iter)) \n",
    "    \n",
    "\n",
    "    def _load_untargeted_wmdp_data(self) -> tuple[list[str], list[list[str]]]:\n",
    "        \"\"\"\n",
    "        For untargeted data loading and SFT.\n",
    "        Use wmdp cut utils, but don't return retain_data_list since we load wikitext separately.\n",
    "\n",
    "        Returns:\n",
    "            keywords_list: List of keywords for making control vectors in CUT for each dataset.\n",
    "            forget_data_list: List of lists, where outer list corresponds to each dataset, \n",
    "                and inner list is the data for that dataset.\n",
    "            retain_data_list: List of lists, where outer list corresponds to each dataset,\n",
    "                and inner list is the data for that dataset.\n",
    "                For untargeted, this is just wikitext, and it isn't even used.\n",
    "        \"\"\"\n",
    "        retain_corpora = self.config.retain_corpora.split(\",\")\n",
    "        forget_corpora = self.config.forget_corpora.split(\",\")\n",
    "        keywords_list, forget_data_list, retain_data_list = get_data(\n",
    "            forget_corpora,\n",
    "            retain_corpora,\n",
    "            self.rmu_config.min_len,\n",
    "            self.rmu_config.max_len,\n",
    "            self.rmu_config.batch_size,\n",
    "        )\n",
    "        return forget_data_list\n",
    "    \n",
    "\n",
    "    def _load_sft_dataset(self) -> list[str]:\n",
    "        \"\"\"Works for wikitext and alpaca.\"\"\"\n",
    "        if self.config.sft_corpora == \"wikitext\":\n",
    "                raw_data = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "        elif self.config.sft_corpora == \"alpaca\":\n",
    "                raw_data = load_dataset(\"tatsu-lab/alpaca\", \"default\", split=\"train\")\n",
    "        data = []\n",
    "        for x in raw_data:\n",
    "            if len(x['text']) > self.config.rmu_config.min_len:\n",
    "                data.append(str(x['text'][:self.config.rmu_config.max_len]))\n",
    "        return data\n",
    "    \n",
    "\n",
    "    def _make_untargeted_wmdp_dataloaders(\n",
    "        self, \n",
    "        data: Union[List[str], List[List[str]]], \n",
    "        tokenizer: AutoTokenizer, \n",
    "        sft: Optional[bool] = False,\n",
    "    ) -> Union[DataLoader, tuple[DataLoader, DataLoader]]:\n",
    "        \"\"\"\n",
    "        A very baked in and non generic function.\n",
    "        Don't use for anything except WMDP exactly as loaded with tuple list [bio, cyber].\n",
    "\n",
    "        Args:\n",
    "            sft: Flag, if set True: means that the data list is just multiple copies of the same dataset.\n",
    "                This is a relic of the get_data function from the CUT codebase, and probably can be refactored.\n",
    "                This will only be used to generate supervised fine tuning dataloader for SFT portion of R2D2 loss in LAT.\n",
    "        Returns:\n",
    "            Dataloaders.\n",
    "        \"\"\"\n",
    "        if sft:\n",
    "            return DataLoader(\n",
    "                data, \n",
    "                shuffle=True, \n",
    "                batch_size=self.config.sft_batch_size, \n",
    "                collate_fn=WMDPLATDataCollator(tokenizer, truncate_length=self.config.data_truncate_length), \n",
    "                drop_last=True,\n",
    "            )\n",
    "            \n",
    "        wmdp_bio_dataloader = DataLoader(\n",
    "            data[0], \n",
    "            shuffle=True, \n",
    "            batch_size=self.config.lat_batch_size, \n",
    "            collate_fn=WMDPLATDataCollator(tokenizer, truncate_length=self.config.data_truncate_length), \n",
    "            drop_last=True,\n",
    "        )\n",
    "        wmdp_cyber_dataloader = DataLoader(\n",
    "            data[1], \n",
    "            shuffle=True, \n",
    "            batch_size=self.config.lat_batch_size, \n",
    "            collate_fn=WMDPLATDataCollator(tokenizer, truncate_length=self.config.data_truncate_length), \n",
    "            drop_last=True,\n",
    "        )\n",
    "        return wmdp_bio_dataloader, wmdp_cyber_dataloader\n",
    "\n",
    "\n",
    "    def _load_targeted_wmdp_data(self) -> tuple[Dataset, Dataset]:\n",
    "        \"\"\"\n",
    "        Use both retain and unlearn WMDP datasets.\n",
    "        Valid reteain keywords: all wikitext or all bio/cyber retain. Not allowed to mix wikitext and WMDP retain.\n",
    "        \n",
    "        Returns:\n",
    "            keywords_list: List of keywords for making control vectors in CUT for each dataset.\n",
    "            bio_dataset: Concatenated dataset for bio, containing adv_labels and def_labels columns.\n",
    "            cyber_dataset: Concatenated dataset for cyber, containing adv_labels and def_labels columns.\n",
    "        \"\"\"\n",
    "        retain_corpora = self.config.retain_corpora.split(\",\")\n",
    "        forget_corpora = self.config.forget_corpora.split(\",\")\n",
    "\n",
    "        # Load and rename datasets for 'forget' corpora\n",
    "        # Only bio forget needs to be locally loaded\n",
    "        hf_location = \"cais/wmdp-corpora\"\n",
    "        for d in forget_corpora:\n",
    "            dataset_path = f\"wmdp/data/{d}.jsonl\"\n",
    "            if d == \"bio-forget-corpus\":\n",
    "                bio_forget = load_dataset('json', data_files=dataset_path, split='train')\n",
    "                bio_forget = bio_forget.rename_column('text', 'adv_tokens')\n",
    "            elif d == \"cyber-forget-corpus\":\n",
    "                cyber_forget = load_dataset(hf_location, name=d, split='train')\n",
    "                cyber_forget = cyber_forget.rename_column('text', 'adv_tokens')\n",
    "\n",
    "        if retain_corpora == ['bio-retain-corpus', 'cyber-retain-corpus']:\n",
    "            # Load and rename datasets for 'retain' corpora\n",
    "            for d in retain_corpora:\n",
    "                retain_dataset = load_dataset(hf_location, name=d, split='train')\n",
    "                retain_dataset = retain_dataset.rename_column('text', 'def_tokens')\n",
    "                if d == \"bio-retain-corpus\":\n",
    "                    bio_retain = retain_dataset\n",
    "                elif d == \"cyber-retain-corpus\":\n",
    "                    cyber_retain = retain_dataset\n",
    "        elif retain_corpora == ['wikitext', 'wikitext']:\n",
    "            bio_retain = cyber_retain = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "            bio_retain = bio_retain.rename_column('text', 'def_tokens')\n",
    "            cyber_retain = cyber_retain.rename_column('text', 'def_tokens')\n",
    "\n",
    "        def merge_rows(example1, example2):\n",
    "            return {'adv_tokens': example1['adv_tokens'], 'def_tokens': example2['def_tokens']}\n",
    "\n",
    "        min_bio_length, min_cyber_length = min(len(bio_forget), len(bio_retain)), min(len(cyber_forget), len(cyber_retain))\n",
    "        bio_dataset = bio_forget.select(range(min_bio_length)).map(\n",
    "            lambda x, idx: merge_rows(x, bio_retain[idx]),\n",
    "            with_indices=True,\n",
    "        )\n",
    "        cyber_dataset = cyber_forget.select(range(min_cyber_length)).map(\n",
    "            lambda x, \n",
    "            idx: merge_rows(x, cyber_retain[idx]), \n",
    "            with_indices=True\n",
    "        )\n",
    "        bio_dataset = bio_dataset.remove_columns(['title', 'abstract', 'doi'])\n",
    "\n",
    "        return bio_dataset, cyber_dataset\n",
    "    \n",
    "\n",
    "    def _make_targeted_wmdp_dataloaders(\n",
    "        self, \n",
    "        bio_dataset: Dataset,\n",
    "        cyber_dataset: Dataset, \n",
    "        tokenizer: AutoTokenizer, \n",
    "    ) -> Union[DataLoader, tuple[DataLoader, DataLoader]]:\n",
    "        \"\"\"\n",
    "        A very baked in and non generic function.\n",
    "        Don't use for anything except WMDP exactly as loaded with tuple list [bio, cyber].\n",
    "\n",
    "        Args:\n",
    "            sft: Flag, if set True: means that the data list is just multiple copies of the same dataset.\n",
    "                This is a relic of the get_data function from the CUT codebase, and probably can be refactored.\n",
    "                This will only be used to generate supervised fine tuning dataloader for SFT portion of R2D2 loss in LAT.\n",
    "        Returns:\n",
    "            Dataloaders.\n",
    "        \"\"\"\n",
    "        wmdp_bio_dataloader = DataLoader(\n",
    "            bio_dataset, \n",
    "            shuffle=True, \n",
    "            batch_size=self.config.lat_batch_size, \n",
    "            collate_fn=WMDPLATTargetedDataCollator(tokenizer, truncate_length=self.config.data_truncate_length), \n",
    "            drop_last=True,\n",
    "        )\n",
    "        wmdp_cyber_dataloader = DataLoader(\n",
    "            cyber_dataset, \n",
    "            shuffle=True, \n",
    "            batch_size=self.config.lat_batch_size, \n",
    "            collate_fn=WMDPLATTargetedDataCollator(tokenizer, truncate_length=self.config.data_truncate_length), \n",
    "            drop_last=True,\n",
    "        )\n",
    "        return wmdp_bio_dataloader, wmdp_cyber_dataloader\n",
    "    \n",
    "\n",
    "    # def _evaluate(self, idx) -> dict[str, float]:\n",
    "    #     \"\"\"Capabilities and WMDP.\"\"\"\n",
    "    #     wmdp_scores = {}\n",
    "    #     wmdp_datasets = [\"chem\", \"bio\", \"cyber\"]\n",
    "    #     for dataset_name in wmdp_datasets:\n",
    "    #         wmdp_mcq = WMDPTask(dataset_name)\n",
    "    #         wmdp_scores[f\"wmdp_{dataset_name}\"] = wmdp_mcq.get_accuracy(self.model, self.tokenizer, batch_size=self.config.minibatch_size, n_batches=50) - 0.25\n",
    "    #     avg_wmdp_score = sum([abs(v) for v in wmdp_scores.values()])/len(wmdp_scores)\n",
    "\n",
    "    #     # Evaluate everything less frequently\n",
    "    #     if idx % (10 * self.config.eval_steps) == 0:\n",
    "    #         capabilities: dict = run_general_evals(self.model, model_type=\"zephyr\")\n",
    "    #         avg_capabilities = sum(capabilities.values())/len(capabilities)\n",
    "    #         combined_evals = capabilities | wmdp_scores | {\"pareto_score\": avg_capabilities / avg_wmdp_score + avg_capabilities}\n",
    "    #     else:\n",
    "    #         mmlu = MMLUTask()\n",
    "    #         mmlu = mmlu.get_accuracy(self.model, tokenizer=self.tokenizer, temperature=0, batch_size=25, n_batches=40, verbose=False)\n",
    "    #         combined_evals = wmdp_scores | {\"MMLU\": mmlu} | {\"pareto_score\": mmlu/ avg_wmdp_score}\n",
    "\n",
    "    #     return combined_evals\n",
    "\n",
    "    def _evaluate_phillip(self, model: nn.Module) -> dict:\n",
    "        \"\"\"Create right padded tokenizer specific to this evaluation method.\"\"\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"HuggingFaceH4/zephyr-7b-beta\", trust_remote_code=True, use_fast=False\n",
    "        )\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        tokenizer.padding_side = \"right\"\n",
    "        tokenizer.truncation_side = \"right\"\n",
    "        tokenizer.mask_token_id = tokenizer.eos_token_id\n",
    "        tokenizer.sep_token_id = tokenizer.eos_token_id\n",
    "        tokenizer.cls_token_id = tokenizer.eos_token_id\n",
    "\n",
    "        wmdp_scores = {}\n",
    "        num_iters = 10\n",
    "        subtasks = [\"wmdp-bio\", \"wmdp-chem\", \"wmdp-cyber\"]\n",
    "        for subtask in subtasks:\n",
    "            wmdp = WMDP_MCTask(batch_size=16, tokenizer=tokenizer, subset=subtask)\n",
    "            unlearned_accuracy = 0\n",
    "            for _ in tqdm(range(num_iters)):\n",
    "                unlearned_accuracy += wmdp.get_test_accuracy(model)\n",
    "            wmdp_scores[subtask] = unlearned_accuracy / num_iters\n",
    "        avg_wmdp_score = sum([abs(v) for v in wmdp_scores.values()])/len(wmdp_scores)\n",
    "        print(wmdp_scores)\n",
    "\n",
    "        # Evaluate everything less frequently\n",
    "        if self.idx % (10 * self.config.eval_steps) == 0:\n",
    "            capabilities: dict = run_general_evals(self.model, model_type=\"zephyr\")\n",
    "            avg_capabilities = sum(capabilities.values())/len(capabilities)\n",
    "            combined_evals = capabilities | wmdp_scores | {\"pareto_score\": avg_capabilities / avg_wmdp_score + avg_capabilities}\n",
    "        else:\n",
    "            mmlu = MMLUTask()\n",
    "            mmlu = mmlu.get_accuracy(self.model, tokenizer=self.tokenizer, temperature=0, batch_size=25, n_batches=40, verbose=False)\n",
    "            combined_evals = wmdp_scores | {\"MMLU\": mmlu} | {\"pareto_score\": mmlu/ avg_wmdp_score}\n",
    "\n",
    "        return combined_evals\n",
    "\n",
    "\n",
    "    def _evaluate_harness(self, is_sample=True) -> dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate with lm-evaluation-harness.\n",
    "        Involves saving peft model.\n",
    "\n",
    "        Format of output from lm_eval.simple_evaluate:\n",
    "        {'wmdp_bio': {'acc,none': 0.5553809897879026, 'acc_stderr,none': 0.01393304254299126, 'alias': 'wmdp_bio'}}\n",
    "        \"\"\"\n",
    "        self.model.save_pretrained(f\"wmdp_models/{self.run_name}\")\n",
    "        model_to_eval = HFLM(\n",
    "            pretrained=model_dict[self.config.model_name],\n",
    "            peft=f\"wmdp_models/{self.run_name}\",\n",
    "            dtype=torch.bfloat16,\n",
    "            device=\"auto\"\n",
    "        )\n",
    "        if is_sample:\n",
    "            eval_func = partial(\n",
    "                lm_eval.simple_evaluate, \n",
    "                model=model_to_eval, \n",
    "                batch_size=16, \n",
    "                limit=16, \n",
    "                cache_requests=True\n",
    "            )\n",
    "        else:\n",
    "            eval_func = partial(\n",
    "                lm_eval.simple_evaluate, \n",
    "                model=model_to_eval, \n",
    "                batch_size=512, \n",
    "                cache_requests=True\n",
    "            )\n",
    "        \n",
    "        # WMDP\n",
    "        wmdp_scores_raw: dict[str, dict[str, float]] = eval_func(tasks=[\"wmdp_bio\", \"wmdp_chem\", \"wmdp_cyber\"])\n",
    "        wmdp_scores = {k: v[\"acc,none\"] for k, v in wmdp_scores_raw[\"results\"].items()}\n",
    "        if is_sample:\n",
    "            wmdp_vars = {f\"{k}_var\": v[\"acc_stderr,none\"] for k, v in wmdp_scores_raw[\"results\"].items()}\n",
    "            wandb.log(wmdp_vars, step=self.idx)\n",
    "        avg_wmdp_score = sum(wmdp_scores.values())/3\n",
    "\n",
    "        # Capabilities and side effects\n",
    "        if is_sample:\n",
    "            mmlu_raw = eval_func(tasks=[\"mmlu\"])\n",
    "            mmlu_acc = {\"mmlu\": mmlu_raw[\"results\"][\"mmlu\"][\"acc,none\"]}\n",
    "            mmlu_var = {f\"MMLU_var\": mmlu_raw[\"results\"][\"mmlu\"][\"acc_stderr,none\"]}\n",
    "            wandb.log(mmlu_var, step=self.idx)\n",
    "            combined_evals = wmdp_scores | mmlu_acc | {\"pareto_score\": mmlu_acc[\"mmlu\"] / avg_wmdp_score}\n",
    "        else:\n",
    "            capabilities_raw: dict = eval_func(tasks=[\"mmlu\", \"sciq\", \"hellaswag\", \"piqa\", \"lambada\", \"winogrande\", \"agieval\", \"mmlu_virology\", \"mmlu_college_computer_science\", \"mmlu_high_school_computer_science\", \"mmlu_college_biology\", \"mmlu_high_school_biology\"])\n",
    "            capabilities = {k: v[\"acc,none\"] for k, v in capabilities_raw[\"results\"].items()}\n",
    "            avg_capabilities = sum(capabilities.values())/len(capabilities)\n",
    "            combined_evals = capabilities | wmdp_scores | {\"pareto_score\": avg_capabilities / avg_wmdp_score + avg_capabilities}\n",
    "            \n",
    "        del model_to_eval\n",
    "        return combined_evals\n",
    "    \n",
    "\n",
    "    def _load_model(self, hf_api_key: str) -> tuple[nn.Module, AutoTokenizer]:\n",
    "        \"\"\"Model and tokenizer loading specifically for WMDP benchmark.\"\"\"\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_dict[self.config.model_name],\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            token=hf_api_key,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_dict[self.config.model_name],\n",
    "            token=hf_api_key,\n",
    "            trust_remote_code=True,\n",
    "            use_fast=False,\n",
    "        )\n",
    "\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        tokenizer.padding_side = \"left\"\n",
    "        tokenizer.mask_token_id = tokenizer.eos_token_id\n",
    "        tokenizer.sep_token_id = tokenizer.eos_token_id\n",
    "        tokenizer.cls_token_id = tokenizer.eos_token_id\n",
    "        \n",
    "        peft_config = LoraConfig(\n",
    "            r=self.config.lora_config.lora_rank,\n",
    "            target_modules=OmegaConf.to_container(self.config.lora_config.lora_target_modules),\n",
    "        )\n",
    "        model = get_peft_model(model, peft_config)\n",
    "        print(f\"PEFT model is on {self.device}\")\n",
    "        return model, tokenizer\n",
    "    \n",
    "\n",
    "    def _set_logger(self) -> None:\n",
    "        \"\"\"WandB as default.\"\"\"\n",
    "        logging.info(f\"Hydra current working directory: {os.getcwd()}\")\n",
    "        logger_params = {\n",
    "            # \"name\": self.run_name, # Had to get rid of this to make it work with sweeps\n",
    "            \"project\": self.config.wandb_config.wandb_project_name,\n",
    "            \"settings\": wandb.Settings(start_method=\"thread\"),\n",
    "            \"config\": OmegaConf.to_container(self.config, resolve=True, throw_on_missing=True),\n",
    "            \"mode\": \"disabled\" if not self.config.wandb_config.log_to_wandb else \"online\",\n",
    "            \"entity\": self.config.wandb_config.entity_name,\n",
    "        }\n",
    "        run = wandb.init(**logger_params)\n",
    "        return run\n",
    "\n",
    "\n",
    "    def _disable_model_gradients(self) -> None:\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad_(False)\n",
    "    \n",
    "\n",
    "    def _enable_model_gradients(self) -> None:\n",
    "        n_layers = self.model.config.num_hidden_layers\n",
    "        for i in range(n_layers):\n",
    "            if i in self.model_layers:\n",
    "                if self.only_train_lora:\n",
    "                    for name, param in self.model.get_submodule(self.config.pgd_config.model_layers_module)[i].named_parameters():\n",
    "                        if \"lora_\" in name:\n",
    "                            param.requires_grad_(True)\n",
    "                else:\n",
    "                    self.model.get_submodule(self.config.pgd_config.model_layers_module)[i].requires_grad_(True)\n",
    "    \n",
    "\n",
    "    def _train_defence(\n",
    "        self, \n",
    "        batch: dict[str, torch.Tensor], \n",
    "        hooks: list[CustomHook], \n",
    "        sft_batch: dict[str, torch.Tensor],\n",
    "        zero_grad: bool,\n",
    "        do_grad_step: bool,\n",
    "    ) -> dict[str, float]:\n",
    "        # Initialize optimizer and loss\n",
    "        losses = {}\n",
    "        if zero_grad:\n",
    "            self.def_optim.zero_grad()\n",
    "\n",
    "        # Compute the defense        \n",
    "        do_defense_step(\n",
    "            model=self.model,\n",
    "            batch=batch,\n",
    "            losses_dict=losses,\n",
    "            wrappers=hooks,\n",
    "            sft_batch=sft_batch,\n",
    "            coefs=self.config.def_config.def_loss_coefs,\n",
    "            log_loss=do_grad_step,\n",
    "            device=self.device,\n",
    "            towards_loss_type=self.config.def_config.towards_loss_type,\n",
    "            away_loss_type=self.config.def_config.away_loss_type,\n",
    "        )\n",
    "        zero_nan_grads(self.model)\n",
    "        # Do gradient step\n",
    "        if do_grad_step:\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.clip_grad)\n",
    "            self.def_optim.step()\n",
    "\n",
    "        return losses\n",
    "    \n",
    "\n",
    "    def _train_attack(\n",
    "        self, \n",
    "        batch: dict[str, torch.Tensor], \n",
    "        do_grad_step: bool,\n",
    "        pca_kwargs: dict = None,\n",
    "    ) -> None:\n",
    "        config = self.config.pgd_config\n",
    "        return projected_gradient_descent(\n",
    "            batch=batch,\n",
    "            model=self.model,\n",
    "            model_layers_module=config.model_layers_module,\n",
    "            layer_list=config.pgd_layers,\n",
    "            epsilon=config.epsilon,\n",
    "            learning_rate=config.inner_learning_rate,\n",
    "            pgd_iterations=config.pgd_iterations_per_step,\n",
    "            loss_coefs=config.adv_loss_coefs,\n",
    "            log_loss=do_grad_step,\n",
    "            device=self.device,\n",
    "            use_cut=self.config.use_cut,\n",
    "            towards_loss_type=config.towards_loss_type,\n",
    "            away_loss_type=config.away_loss_type,\n",
    "            pca_kwargs=pca_kwargs,\n",
    "        )\n",
    "    \n",
    "\n",
    "    def _lat_training_step(\n",
    "        self, \n",
    "        batch: dict[str, torch.Tensor],\n",
    "        sft_batch: Optional[dict[str, torch.Tensor]] = None,\n",
    "        pca_kwargs: dict = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Make minibatches and train with gradient accumulation, or train batches directly.\"\"\"\n",
    "        # Gradient accumulation\n",
    "        if not self.config.minibatch_size == self.config.lat_batch_size:\n",
    "            minibatch_steps = range(0, self.config.lat_batch_size, self.config.minibatch_size)\n",
    "            adv_hooks = []\n",
    "\n",
    "            # Train adversary with minibatch accumulation\n",
    "            for start_idx in minibatch_steps:\n",
    "                curr_batch = get_minibatch(batch, start_idx, self.config.minibatch_size)\n",
    "\n",
    "                self._disable_model_gradients()\n",
    "                losses, hooks, _ = self._train_attack(\n",
    "                    batch=curr_batch,\n",
    "                    do_grad_step=start_idx==minibatch_steps[-1],\n",
    "                    pca_kwargs=pca_kwargs,\n",
    "                )\n",
    "                adv_hooks.append(hooks)\n",
    "                # Disable adversary for this minibatch\n",
    "                for hook in hooks:\n",
    "                    hook.enabled = False\n",
    "            # Log last value to WandB\n",
    "            if self.post_adv_callback is not None:\n",
    "                self.post_adv_callback(result=losses, epoch=self.idx)\n",
    "            \n",
    "            # Train defence with minibatch accumulation\n",
    "            for _ in range(self.config.def_config.model_iterations_per_step):\n",
    "                for i, start_idx in enumerate(minibatch_steps):\n",
    "                    curr_batch = get_minibatch(batch, start_idx, self.config.minibatch_size)\n",
    "                    curr_sft_batch = get_minibatch(sft_batch, start_idx, self.config.minibatch_size) if self.config.def_config.def_loss_coefs.sft > 0 else None\n",
    "                    hooks = adv_hooks[i]\n",
    "                    # Enable adversary for this minibatch\n",
    "                    for hook in hooks:\n",
    "                        hook.enabled = True\n",
    "                    self._enable_model_gradients()\n",
    "                    def_losses = self._train_defence(\n",
    "                        batch=curr_batch,\n",
    "                        hooks=hooks,\n",
    "                        sft_batch=curr_sft_batch,\n",
    "                        zero_grad=start_idx==0,\n",
    "                        do_grad_step=start_idx==minibatch_steps[-1],\n",
    "                    )\n",
    "                    for hook in hooks:\n",
    "                        hook.enabled = False\n",
    "        \n",
    "        # No gradient accumulation\n",
    "        else:\n",
    "            # Train adversary\n",
    "            self._disable_model_gradients()\n",
    "            losses, hooks, _ = self._train_attack(\n",
    "                batch=batch,\n",
    "                do_grad_step=True,\n",
    "            )\n",
    "            if self.post_adv_callback is not None:\n",
    "                self.post_adv_callback(result=losses, epoch=self.idx)\n",
    "\n",
    "            # Train defence\n",
    "            self._enable_model_gradients()\n",
    "            for _ in range(self.config.def_config.model_iterations_per_step):\n",
    "                def_losses = self._train_defence(\n",
    "                    batch=batch,\n",
    "                    hooks=hooks,\n",
    "                    sft_batch=sft_batch,\n",
    "                    zero_grad=True,\n",
    "                    do_grad_step=True,\n",
    "                )\n",
    "        \n",
    "        # Log results\n",
    "        losses.update(def_losses)\n",
    "        if self.post_def_callback is not None:\n",
    "            self.post_def_callback(result=def_losses, epoch=self.idx)\n",
    "        clear_hooks(self.model)\n",
    "\n",
    "\n",
    "    def train(self) -> None:\n",
    "        \"\"\"\n",
    "        Main training loop for WMDP benchmark.\n",
    "        Set logger and call callback functions.\n",
    "        Iterate over num_epochs (which really is just PGD iterations), or until wallclock runs out.\n",
    "            Will always default to infinite training unless wallclock time not specified.\n",
    "        Check if using minibatch training.\n",
    "\n",
    "        An epoch here equates to seeing a single batch of data.\n",
    "        \"\"\"\n",
    "        # Call initialisation callbacks\n",
    "        if self.init_callback is not None:\n",
    "            self.init_callback(result={}, epoch=-1)\n",
    "\n",
    "        start_time = time.time()\n",
    "        self.idx = 0\n",
    "        epochs_iter = tqdm(range(self.config.num_epochs)) if self.config.num_epochs is not None else tqdm(itertools.count())\n",
    "        print(f\"Config epochs {self.config.num_epochs}\")\n",
    "\n",
    "        for epoch_idx in tqdm(epochs_iter):\n",
    "\n",
    "            if self.config.def_config.reinitialize_def_optim:\n",
    "                self.def_optim = torch.optim.AdamW(\n",
    "                    self.model.parameters(),\n",
    "                    lr=self.schedule[self.idx] if self.config.def_config.use_cosine_schedule else self.config.def_config.outer_learning_rate,\n",
    "                )\n",
    "            # batch = next(self.cyber_dataloader) if self.idx % 5 else next(self.bio_dataloader)\n",
    "            batch = next(self.bio_dataloader) # Temporary: USE BIO FOR NOW\n",
    "            sft_batch = next(self.sft_dataloader) if self.sft_dataloader else None\n",
    "\n",
    "            # try:\n",
    "            self._lat_training_step(batch, sft_batch)\n",
    "        \n",
    "            # Log evaluation metrics every 10 its\n",
    "            if self.idx % self.config.eval_steps == 0:\n",
    "                with torch.inference_mode():\n",
    "                    eval_accs = self._evaluate_harness()\n",
    "                eval_and_log(eval_accs, self.idx)\n",
    "\n",
    "            if self.idx % 20 == 0:\n",
    "                self._save_model()\n",
    "            self.idx += 1\n",
    "\n",
    "            # except Exception as e:\n",
    "            #     print(f\"Error at epoch {epoch_idx} of {self.run_name}: {e}\")\n",
    "            #     os.makedirs(\"logs\", exist_ok=True)\n",
    "            #     with open(f\"logs/{self.run_name}_errors.txt\", \"a\") as f:\n",
    "            #         f.write(f\"Error at epoch {epoch_idx} of {self.run_name}: {e}\\n\")\n",
    "            #     break\n",
    "\n",
    "            elapsed_time = time.time() - start_time\n",
    "            if self.config.time_limit is not None and elapsed_time > self.config.time_limit:\n",
    "                print(f\"Reached {elapsed_time} seconds at epoch {epoch_idx}\")\n",
    "                break\n",
    "        \n",
    "        self._save_model()\n",
    "        self._evaluate_harness(is_sample=False)\n",
    "        for attr in list(self.__dict__.keys()):\n",
    "            del attr\n",
    "        torch.cuda.empty_cache()\n",
    "        wandb.finish()\n",
    "    \n",
    "\n",
    "    def _save_model(self):\n",
    "        path = f\"wmdp_models/{self.run_name}\"\n",
    "        clear_hooks(self.model)\n",
    "        self.model.save_pretrained(path)\n",
    "        self.tokenizer.save_pretrained(path)\n",
    "        print(f\"Saved model to {path}\")\n",
    "\n",
    "\n",
    "class RunLATCUT(RunLAT):\n",
    "    def __init__(\n",
    "        self, \n",
    "        config: OmegaConf, \n",
    "        cuda_id: int,\n",
    "        sweep_params: Optional[list[str]],\n",
    "        init_callback: Optional[Callable] = None,\n",
    "        post_def_callback: Optional[Callable] = None,\n",
    "        post_adv_callback: Optional[Callable] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cuda_id: Each Run instance associated with a single GPU. Allows sweeps to call multiple Run instances on multiple GPUs.\n",
    "            sweep_params: Any parameters that are changed by the sweep.\n",
    "            init_callback: Typically, an evaluation function.\n",
    "            post_def_callback: \"\".\n",
    "            post_adv_callback: \"\".\n",
    "        \"\"\"\n",
    "        assert config.use_cut, \"This is a CUT only script.\"\n",
    "        assert isinstance(config.pgd_config.pgd_layers, int), \"pgd_layers must be an integer for CUT as lists are not yet supported\"\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(f\"cuda:{cuda_id}\")\n",
    "        elif torch.backends.mps.is_available():\n",
    "            self.device = torch.device(\"mps\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "        \n",
    "        self.config = config\n",
    "\n",
    "        self.def_coefs = config.def_config.def_loss_coefs\n",
    "\n",
    "        if config.only_train_lora is None:\n",
    "            self.only_train_lora = isinstance(self.model, PeftModel)\n",
    "        else:\n",
    "            self.only_train_lora = config.only_train_lora\n",
    "        \n",
    "        # Callbacks\n",
    "        self.init_callback = init_callback\n",
    "        self.post_adv_callback = post_adv_callback\n",
    "        self.post_def_callback = post_def_callback\n",
    "\n",
    "        # Model\n",
    "        self.model, self.tokenizer = self._load_model(hf_api_key, use_peft=config.use_peft, frozen_model=False)\n",
    "        self.frozen_model, _ = self._load_model(hf_api_key, frozen_model=True)\n",
    "\n",
    "        if config.replicate_cut:\n",
    "            self.forget_data_list, self.retain_data_list = get_data(\n",
    "                config.rmu_config.forget_corpora,\n",
    "                config.rmu_config.retain_corpora,\n",
    "                config.rmu_config.min_len,\n",
    "                config.rmu_config.max_len,\n",
    "                config.rmu_config.batch_size,\n",
    "            )\n",
    "            # TODO: uncommented below block\n",
    "            bio_dataset, cyber_dataset = self._load_targeted_wmdp_data()\n",
    "            self.bio_dataloader, self.cyber_dataloader = self._make_targeted_wmdp_dataloaders(bio_dataset, cyber_dataset, self.tokenizer)\n",
    "            self.bio_dataloader, self.cyber_dataloader = itertools.cycle(self.bio_dataloader), itertools.cycle(self.cyber_dataloader)\n",
    "            self.model_layers = self.config.model_layers if self.config.model_layers is not None else range(self.model.config.num_hidden_layers)\n",
    "\n",
    "            self.run_rmu_copy(args=config.rmu_config)\n",
    "\n",
    "        else:\n",
    "            if config.use_untargeted: # Untargeted LAT, use unlearn for defence and adversary\n",
    "                forget_list = self._load_untargeted_wmdp_data()\n",
    "                self.bio_dataloader, self.cyber_dataloader = self._make_untargeted_wmdp_dataloaders(forget_list, self.tokenizer, sft=False)\n",
    "            else: # Use W           MDP retain and unlearn - note logic is *totally different* to above\n",
    "                assert config.forget_corpora == 'bio-forget-corpus,cyber-forget-corpus'\n",
    "                bio_dataset, cyber_dataset = self._load_targeted_wmdp_data()\n",
    "                self.bio_dataloader, self.cyber_dataloader = self._make_targeted_wmdp_dataloaders(bio_dataset, cyber_dataset, self.tokenizer)\n",
    "            self.bio_dataloader, self.cyber_dataloader = itertools.cycle(self.bio_dataloader), itertools.cycle(self.cyber_dataloader)\n",
    "            # SFT dataloader\n",
    "            if config.def_config.def_loss_coefs.sft > 0:\n",
    "                sft_dataset: list[str] = self._load_sft_dataset()\n",
    "                self.sft_dataloader = self._make_untargeted_wmdp_dataloaders(sft_dataset, self.tokenizer, sft=True)\n",
    "                self.sft_dataloader = itertools.cycle(self.sft_dataloader)\n",
    "            else:\n",
    "                assert config.def_config.def_loss_coefs[\"sft\"] == 0\n",
    "                self.sft_dataloader = None\n",
    "            assert next(self.bio_dataloader)[\"def_tokens\"].shape[0] == self.config.lat_batch_size, \"Batch size mismatch\"\n",
    "\n",
    "            # Put optimizer for defence model here \n",
    "            # PGD optim initialized in projected_gradient_descent function\n",
    "            self.def_optim = torch.optim.Adam(\n",
    "                get_params(self.model, layer_ids=OmegaConf.to_container(self.config.rmu_config.layer_ids), param_ids=self.config.rmu_config.param_ids), \n",
    "                lr=config.def_config.outer_learning_rate\n",
    "            )\n",
    "            # self.def_optim = torch.optim.Adam(self.model.parameters(), lr=config.def_config.outer_learning_rate)\n",
    "\n",
    "            run = self._set_logger()\n",
    "            if sweep_params: # For wandb sweeps: update with wandb values\n",
    "                config = update_with_wandb_config(config, sweep_params)\n",
    "            self.run_name = f\"{config.model_name.lower()}_layers{config.pgd_config.pgd_layers}_alpha{config.rmu_config.alpha}_ccoef{config.rmu_config.steering_coef}_{config.wandb_config.additional_run_name}_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "            run.name = self.run_name\n",
    "\n",
    "            self.train_cut()\n",
    "\n",
    "\n",
    "    def _load_untargeted_wmdp_data(self) -> tuple[list[str], list[list[str]]]:\n",
    "        \"\"\"\n",
    "        For untargeted data loading and SFT.\n",
    "        Use wmdp cut utils, but don't return retain_data_list since we load wikitext separately.\n",
    "\n",
    "        Returns:\n",
    "            keywords_list: List of keywords for making control vectors in CUT for each dataset.\n",
    "            forget_data_list: List of lists, where outer list corresponds to each dataset, \n",
    "                and inner list is the data for that dataset.\n",
    "            retain_data_list: List of lists, where outer list corresponds to each dataset,\n",
    "                and inner list is the data for that dataset.\n",
    "                For untargeted, this is just wikitext, and it isn't even used.\n",
    "        \"\"\"\n",
    "        retain_corpora = self.config.retain_corpora.split(\",\")\n",
    "        forget_corpora = self.config.forget_corpora.split(\",\")\n",
    "        keywords_list, forget_data_list, retain_data_list = get_data(\n",
    "            forget_corpora,\n",
    "            retain_corpora,\n",
    "            self.config.rmu_config.min_len,\n",
    "            self.config.rmu_config.max_len,\n",
    "            self.config.rmu_config.batch_size,\n",
    "        )\n",
    "        return forget_data_list, retain_data_list\n",
    "    \n",
    "\n",
    "    def _make_rmu_vec(self, batch, module):\n",
    "        \"\"\"\n",
    "        Make uniform random drawn from 0-1 attack vector.\n",
    "        Call once at start of training loop.\n",
    "        \"\"\"\n",
    "        activations = forward_with_cache(\n",
    "            self.frozen_model, batch[\"def_tokens\"].to(self.device), module=module, no_grad=True\n",
    "        ).to(self.device)\n",
    "        random_vector = torch.rand(activations.shape, device=self.device)\n",
    "        self.rmu_vec = self._norm_attacks(random_vector)\n",
    "\n",
    "        # # Allow size (1,1,hidden) to broadcast later\n",
    "        # random_vector = torch.rand(1, 1, self.model.config.hidden_size, device=self.device)\n",
    "        # self.rmu_vec = random_vector / torch.norm(random_vector)\n",
    "\n",
    "\n",
    "    def _load_model(self, hf_api_key: str, use_peft: Optional[bool] = False, frozen_model: Optional[bool] = False):\n",
    "        \"\"\"Model and tokenizer loading specifically for WMDP benchmark.\n",
    "        \n",
    "        Key for saving compute is PEFT config can be loaded and unloaded.\n",
    "        \"\"\"\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_dict[self.config.model_name],\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            token=hf_api_key,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\",\n",
    "        ).to(self.device)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_dict[self.config.model_name],\n",
    "            token=hf_api_key,\n",
    "            trust_remote_code=True,\n",
    "            use_fast=False,\n",
    "        )\n",
    "\n",
    "        if not frozen_model:\n",
    "            tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "            tokenizer.padding_side = \"right\"\n",
    "            tokenizer.mask_token_id = tokenizer.eos_token_id\n",
    "            tokenizer.sep_token_id = tokenizer.eos_token_id\n",
    "            tokenizer.cls_token_id = tokenizer.eos_token_id\n",
    "        \n",
    "            if use_peft:\n",
    "                peft_config = LoraConfig(\n",
    "                    r=self.config.lora_config.lora_rank,\n",
    "                    target_modules=OmegaConf.to_container(self.config.lora_config.lora_target_modules),\n",
    "                )\n",
    "                model = get_peft_model(model, peft_config)\n",
    "                print(f\"PEFT model is on {self.device}\")\n",
    "            else: # Don't use all layers, only some layers full finetune\n",
    "                self.model = model\n",
    "                print(f\"Model without PEFT is on {self.device}\")\n",
    "        \n",
    "        return model, tokenizer\n",
    "\n",
    "\n",
    "    def _norm_attacks(self, attacks: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Normalize each 'attack' tensor such that attacks[i][j] has norm 1 (per batch example).\n",
    "        \"\"\"\n",
    "        normalized_attacks = attacks / attacks.norm(dim=-1, keepdim=True)\n",
    "        return normalized_attacks\n",
    "\n",
    "\n",
    "    # TODO: make adversary possible on multiple layers (matching multiple layers)\n",
    "    # Currently adversary attacks is a list to support multiple layers but we only use the first\n",
    "    def _train_defence(\n",
    "        self, \n",
    "        batch: dict[str, torch.Tensor], \n",
    "        do_grad_step: bool,\n",
    "        updated_module: nn.Module,\n",
    "        frozen_module: nn.Module,\n",
    "        zero_grad: bool,\n",
    "        log_loss: Optional[bool] = True,\n",
    "        sft_batch: Optional[dict[str, torch.Tensor]] = None, \n",
    "        attacks: Optional[List[torch.Tensor]] = [],\n",
    "        hooks: Optional[list[CustomHook]] = [], \n",
    "    ) -> dict[str, float]:\n",
    "        \"\"\"\n",
    "        Defence training always with adversary perturbations active in this LAT version of RMU.\n",
    "\n",
    "        Modifications made compared to non-CUT version:\n",
    "        - Assume always use toward and away data, but this time to construct the relevant CUT related activation losses.\n",
    "\n",
    "        Modifications to CUT:\n",
    "        - Attacks from adversary added implicitly, and defence now has to match activations, so we don't necessarily need a forget loss.\n",
    "        - SFT batch now optional, since CUT doesn't automatically use it.\n",
    "        \"\"\"\n",
    "        losses = {\"total\": 0}\n",
    "        if zero_grad:\n",
    "            self.def_optim.zero_grad()\n",
    "\n",
    "        # Retain loss\n",
    "        model_towards_tokens = batch[\"def_tokens\"] if \"def_tokens\" in batch else batch[\"tokens\"]\n",
    "        model_towards_tokens = model_towards_tokens[batch[\"def_labels_mask\"]].to(self.device)\n",
    "        updated_activations = forward_with_cache(\n",
    "            self.model, model_towards_tokens, module=updated_module, no_grad=False,\n",
    "        ).to(self.device)\n",
    "        frozen_activations = forward_with_cache(\n",
    "            self.frozen_model, model_towards_tokens, module=frozen_module, no_grad=True\n",
    "        ).to(self.device)\n",
    "        retain_loss = torch.nn.functional.mse_loss(\n",
    "            updated_activations, frozen_activations\n",
    "        )\n",
    "        retain_loss *= self.config.rmu_config.alpha[0]\n",
    "        (self.def_coefs[\"toward\"] * retain_loss).backward()\n",
    "        losses[\"retain\"] = retain_loss.item()\n",
    "        losses[\"total\"] += retain_loss.item()\n",
    "\n",
    "        # Forget loss\n",
    "        model_away_tokens = batch[\"adv_tokens\"] if \"adv_tokens\" in batch else batch[\"tokens\"]\n",
    "        model_away_tokens = model_away_tokens[batch[\"adv_labels_mask\"]].to(self.device)\n",
    "        updated_activations = forward_with_cache(\n",
    "            self.model, model_away_tokens, module=updated_module, no_grad=False\n",
    "        ).to(self.device)\n",
    "        random_vector = torch.rand(updated_activations.shape, device=self.device)\n",
    "        self.rmu_vec = self._norm_attacks(random_vector)\n",
    "        control_vec = (self.config.rmu_config.steering_coef * self.rmu_vec).to(self.device)\n",
    "        unlearn_loss = torch.nn.functional.mse_loss(\n",
    "            updated_activations, control_vec\n",
    "        )\n",
    "        (self.def_coefs[\"away\"] * unlearn_loss).backward()\n",
    "        losses[\"unlearn\"] = unlearn_loss.item()\n",
    "        losses[\"total\"] += unlearn_loss.item()\n",
    "\n",
    "        # if self.def_coefs[\"sft\"] > 0:\n",
    "        #     sft_tokens = sft_batch[\"def_tokens\"].to(self.device) if \"def_tokens\" in sft_batch else sft_batch[\"tokens\"].to(self.device)\n",
    "        #     sft_labels_mask = sft_batch[\"def_labels_mask\"].to(self.device) if \"def_labels_mask\" in batch else torch.ones_like(batch[\"def_labels\"]).to(self.device)\n",
    "\n",
    "        #     for wrapper in hooks:\n",
    "        #         wrapper.enabled = False\n",
    "\n",
    "        #     with torch.autocast(device_type=\"cuda\"):\n",
    "        #         logits = self.model(sft_tokens.to(self.device)).logits\n",
    "        #         final_logits = logits[:, :-1][sft_labels_mask[:, 1:]]\n",
    "        #         flattened_sft_labels = sft_tokens[:, 1:][sft_labels_mask[:, 1:]]\n",
    "        #         sft_loss = nn.functional.cross_entropy(final_logits, flattened_sft_labels)\n",
    "\n",
    "        #     (self.def_coefs[\"sft\"] * sft_loss).backward()\n",
    "        #     losses[\"sft\"] = sft_loss.item()\n",
    "        #     losses[\"total\"] += sft_loss.item()\n",
    "    \n",
    "        #     for wrapper in hooks:\n",
    "        #         wrapper.enabled = True\n",
    "            \n",
    "        zero_nan_grads(self.model)\n",
    "        # Do gradient step\n",
    "        if do_grad_step:\n",
    "            # torch.nn.utils.clip_grad_norm_(\n",
    "            #     self.model.parameters(), self.config.clip_grad)\n",
    "            self.def_optim.step()\n",
    "        \n",
    "        # This will actually only return loss from last loop iteration\n",
    "        if log_loss:\n",
    "            losses_dict = {}\n",
    "            for key in losses:\n",
    "                losses_dict[\"def_\"+key] = losses[key]\n",
    "            \n",
    "        return losses_dict\n",
    "    \n",
    "\n",
    "    def _train_attack(\n",
    "        self, \n",
    "        batch: dict[str, torch.Tensor], \n",
    "        do_grad_step: bool,\n",
    "        pca_kwargs: Optional[dict] = None,\n",
    "    ) -> None:\n",
    "        config = self.config.pgd_config\n",
    "        return projected_gradient_descent(\n",
    "            batch=batch,\n",
    "            model=self.model,\n",
    "            model_layers_module=config.model_layers_module,\n",
    "            layer_list=config.pgd_layers,\n",
    "            epsilon=config.epsilon,\n",
    "            learning_rate=config.inner_learning_rate,\n",
    "            pgd_iterations=config.pgd_iterations_per_step,\n",
    "            loss_coefs=config.adv_loss_coefs,\n",
    "            log_loss=do_grad_step,\n",
    "            device=self.device,\n",
    "            use_cut=self.config.use_cut,\n",
    "            towards_loss_type=config.towards_loss_type,\n",
    "            away_loss_type=config.away_loss_type,\n",
    "            pca_kwargs=pca_kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "    def _lat_training_step(\n",
    "        self, \n",
    "        batch: dict[str, torch.Tensor],\n",
    "        frozen_module: nn.Module,\n",
    "        updated_module: nn.Module,\n",
    "        sft_batch: Optional[dict[str, torch.Tensor]] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Batch comes directly from batch in dataloader, or from minibatch.\"\"\"\n",
    "        assert isinstance(batch, dict), \"Batch is not dict!\"\n",
    "\n",
    "        if not self.config.minibatch_size == self.config.lat_batch_size:\n",
    "            minibatch_steps = range(0, self.config.lat_batch_size, self.config.minibatch_size)\n",
    "            # adv_hooks = []\n",
    "            # adv_attacks = []\n",
    "\n",
    "            # # Train adversary with minibatch accumulation\n",
    "            # for start_idx in minibatch_steps:\n",
    "            #     if start_idx + self.config.minibatch_size > self.config.sft_batch_size: \n",
    "            #         break\n",
    "            #     curr_batch = get_minibatch(batch, start_idx, self.config.minibatch_size)\n",
    "\n",
    "            #     self._disable_model_gradients()\n",
    "            #     losses, hooks, _ = self._train_attack(\n",
    "            #         batch=curr_batch,\n",
    "            #         do_grad_step=start_idx==minibatch_steps[-1],\n",
    "            #     )\n",
    "            #     adv_hooks.append(hooks)\n",
    "\n",
    "            #     # Used for old version of CUT + LAT\n",
    "            #     # adv_attacks.append(attacks)\n",
    "\n",
    "            #     # Log to WandB\n",
    "            #     if self.post_adv_callback is not None:\n",
    "            #         self.post_adv_callback(result=losses, epoch=self.idx)\n",
    "            #     # Disable adversary for this minibatch\n",
    "            #     for hook in hooks:\n",
    "            #         hook.enabled = False\n",
    "            \n",
    "            # Train defence with minibatch accumulation\n",
    "            for _ in range(self.config.def_config.model_iterations_per_step):\n",
    "                for i, start_idx in enumerate(minibatch_steps):\n",
    "                    curr_batch = get_minibatch(batch, start_idx, self.config.minibatch_size)\n",
    "                    curr_sft_batch = get_minibatch(sft_batch, start_idx, self.config.minibatch_size) if self.config.def_config.def_loss_coefs.sft > 0 else None\n",
    "\n",
    "                    # Get RMU steering vector if at start of training\n",
    "                    if self.idx == 0:\n",
    "                        self._make_rmu_vec(curr_batch, updated_module)\n",
    "\n",
    "                    # hooks = adv_hooks[i]\n",
    "                    # Enable adversary for this minibatch\n",
    "                    # for hook in hooks:\n",
    "                    #     hook.enabled = True\n",
    "                    self._enable_model_gradients()\n",
    "                    def_losses = self._train_defence(\n",
    "                        batch=curr_batch,\n",
    "                        # hooks=hooks,\n",
    "                        # attacks=adv_attacks[i],\n",
    "                        sft_batch=curr_sft_batch,\n",
    "                        frozen_module=frozen_module,\n",
    "                        updated_module=updated_module,\n",
    "                        zero_grad=start_idx==0,\n",
    "                        do_grad_step=start_idx==minibatch_steps[-1],\n",
    "                    )\n",
    "                    # for hook in hooks:\n",
    "                    #     hook.enabled = False\n",
    "        \n",
    "        else:\n",
    "            curr_batch, curr_sft_batch = batch, sft_batch\n",
    "\n",
    "            # Train adversary\n",
    "            # self._disable_model_gradients()\n",
    "            # losses, hooks, attacks = self._train_attack(\n",
    "            #     batch=curr_batch,\n",
    "            #     do_grad_step=True,\n",
    "            # )\n",
    "            # if self.post_adv_callback is not None:\n",
    "            #     self.post_adv_callback(result=losses, epoch=self.idx)\n",
    "\n",
    "            # Train defence\n",
    "            # self._enable_model_gradients()\n",
    "            for _ in range(self.config.def_config.model_iterations_per_step):\n",
    "                def_losses = self._train_defence(\n",
    "                    batch=curr_batch,\n",
    "                    # hooks=hooks,\n",
    "                    # attack=attacks,\n",
    "                    sft_batch=curr_sft_batch,\n",
    "                    frozen_module=frozen_module,\n",
    "                    updated_module=updated_module,\n",
    "                    zero_grad=True,\n",
    "                    do_grad_step=True,\n",
    "                )\n",
    "        \n",
    "        # Log results\n",
    "        # losses.update(def_losses)\n",
    "        if self.post_def_callback is not None:\n",
    "            self.post_def_callback(result=def_losses, epoch=self.idx)\n",
    "        clear_hooks(self.model)\n",
    "\n",
    "\n",
    "    def train_cut(self) -> None:\n",
    "        if self.init_callback is not None:\n",
    "            self.init_callback(result={}, epoch=-1)\n",
    "\n",
    "        start_time = time.time()\n",
    "        self.idx = 0\n",
    "        epochs_iter = tqdm(range(self.config.num_epochs)) if self.config.num_epochs is not None else tqdm(itertools.count())\n",
    "        print(f\"Config epochs {self.config.num_epochs}\")\n",
    "        \n",
    "        if self.config.use_peft:\n",
    "            module_str=\"{model_name}.base_model.model.model.layers[{layer_id}]\"\n",
    "        else:\n",
    "            module_str=\"{model_name}.model.layers[{layer_id}]\"\n",
    "        updated_module = eval(\n",
    "            module_str.format(model_name=\"self.model\", layer_id=self.config.pgd_config.pgd_layers+self.config.rmu_config.post_pgd_offset)\n",
    "        )\n",
    "        frozen_module = eval(\n",
    "            module_str.format(model_name=\"self.frozen_model\", layer_id=self.config.pgd_config.pgd_layers)\n",
    "        )\n",
    "\n",
    "        for epoch_idx in tqdm(epochs_iter):\n",
    "            \"\"\"\n",
    "            This doesn't need refactoring to use the setup,\n",
    "            but pgd_layers must be exactly an integer. \n",
    "            \"\"\"\n",
    "            if self.config.def_config.reinitialize_def_optim:\n",
    "                self.def_optim = torch.optim.AdamW(\n",
    "                    self.model.parameters(),\n",
    "                    lr=self.config.def_config.outer_learning_rate\n",
    "                )\n",
    "            batch = next(self.cyber_dataloader) if self.idx % 2 else next(self.bio_dataloader)\n",
    "            sft_batch = next(self.sft_dataloader) if self.sft_dataloader else None\n",
    "            self._lat_training_step(\n",
    "                batch, \n",
    "                sft_batch=sft_batch, \n",
    "                frozen_module=frozen_module, \n",
    "                updated_module=updated_module\n",
    "            )\n",
    "            \n",
    "            # Log evaluation metrics every 10 its\n",
    "            if self.idx % self.config.eval_steps == 0:\n",
    "                with torch.inference_mode():\n",
    "                    eval_accs = self._evaluate_phillip(self.model)\n",
    "                eval_and_log(eval_accs, self.idx)\n",
    "\n",
    "            self.idx += 1\n",
    "\n",
    "            elapsed_time = time.time() - start_time\n",
    "            if self.config.time_limit is not None and elapsed_time > self.config.time_limit:\n",
    "                print(f\"Reached {elapsed_time} seconds at epoch {epoch_idx}\")\n",
    "                break\n",
    "        \n",
    "        # Evaluate using harness at end\n",
    "        self._evaluate_harness()\n",
    "\n",
    "        wandb.finish()\n",
    "    \n",
    "\n",
    "    def _save_model(self):\n",
    "        self.model.save_pretrained(save_directory=f\"wmdp_cutlat_models/{self.run_name}\")\n",
    "\n",
    "\n",
    "    def run_rmu(self, args) -> None:  \n",
    "        \"\"\"\n",
    "        Method directly copied from CAIS RMU unlearn code.\n",
    "        Does not use SFT or PEFT.\n",
    "        Only trains 3 layers before layer 7.\n",
    "        Forget - WMDP unlearn corpora; retain - wikitext.\n",
    "\n",
    "        Args:\n",
    "            args: OmegaConf dictionary-like object with dot access.\n",
    "        \"\"\"\n",
    "        logger_params = {\n",
    "            # \"name\": self.run_name, # Had to get rid of this to make it work with sweeps\n",
    "            \"project\": \"rmu_replicate\",\n",
    "            \"settings\": wandb.Settings(start_method=\"thread\"),\n",
    "            \"config\": OmegaConf.to_container(args),\n",
    "            \"mode\": \"online\"\n",
    "        }\n",
    "        wandb.init(**logger_params, entity=\"quirky_lats_at_mats\")\n",
    "        \n",
    "        rmu_config = vars(args)\n",
    "        print(\"====rmu Config====\")\n",
    "        print(\"\\n\".join(f\"{k}={v}\" for k,v in rmu_config.items()))\n",
    "        print(\"=====\")\n",
    "\n",
    "        self.model = self.model.train()\n",
    "        params = get_params(self.model, args.layer_ids, args.param_ids)\n",
    "        optimizer = torch.optim.AdamW(params, lr=args.lr)\n",
    "        frozen_module = eval(\n",
    "            args.module_str.format(model_name=\"self.frozen_model\", layer_id=args.layer_id)\n",
    "        )\n",
    "        updated_module = eval(\n",
    "            args.module_str.format(model_name=\"self.model\", layer_id=args.layer_id)\n",
    "        )\n",
    "\n",
    "        control_vectors_list = []\n",
    "        for i in range(len(self.forget_data_list)):\n",
    "            random_vector = torch.rand(1,1, self.model.config.hidden_size, device=self.device)\n",
    "            control_vec = random_vector / torch.norm(random_vector) * args.steering_coef\n",
    "            control_vectors_list.append(control_vec)\n",
    "\n",
    "        num_batches = min(\n",
    "            args.max_num_batches,\n",
    "            min([len(f) for f in self.forget_data_list]),\n",
    "            min([len(r) for r in self.retain_data_list]),\n",
    "        )\n",
    "        \n",
    "        truncation_side = self.tokenizer.truncation_side\n",
    "        self.tokenizer.truncation_side=\"right\"\n",
    "\n",
    "        for epoch in range(1):\n",
    "            print(f\"======= Epoch {epoch} =======\")\n",
    "            with tqdm(total=num_batches) as pbar:\n",
    "                for idx in range(num_batches):\n",
    "                    self.idx = idx\n",
    "                    if idx % self.config.rmu_config.eval_freq == 0:\n",
    "                        with torch.inference_mode():\n",
    "                            accs = self._evaluate_phillip(self.model)\n",
    "                        eval_and_log(accs, idx)\n",
    "                    \n",
    "                    topic_idx = idx % len(self.forget_data_list)\n",
    "                    # if not topic_idx:\n",
    "                    #     pgd_batch = next(self.bio_dataloader)\n",
    "                    # else:\n",
    "                    #     pgd_batch = next(self.cyber_dataloader)\n",
    "                    batch_idx = idx // len(self.forget_data_list)\n",
    "                    control_vec = control_vectors_list[topic_idx]\n",
    "                    unlearn_batch = self.forget_data_list[topic_idx][batch_idx]\n",
    "                    retain_batch = self.retain_data_list[topic_idx][batch_idx]\n",
    "\n",
    "                    # # ====== Train adversary PGD ===========\n",
    "                    # self._disable_model_gradients()\n",
    "                    # losses, _, _ = self._train_attack(\n",
    "                    #     batch=pgd_batch,\n",
    "                    #     do_grad_step=True,\n",
    "                    # )\n",
    "                    # if self.post_adv_callback is not None:\n",
    "                    #     self.post_adv_callback(result=losses, epoch=self.idx)\n",
    "                    # self._enable_model_gradients()\n",
    "\n",
    "                    # ===== Train RMU ============\n",
    "                    # Unlearning loss\n",
    "                    max_length = 512 if topic_idx == 0 else 768\n",
    "                    # max_length = self.config.data_truncate_length\n",
    "                    unlearn_inputs = self.tokenizer(\n",
    "                        unlearn_batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length\n",
    "                    ).to(self.device)\n",
    "                    updated_forget_activations = forward_with_cache(\n",
    "                        self.model, unlearn_inputs, module=updated_module, no_grad=False\n",
    "                    ).to(self.device)\n",
    "\n",
    "                    unlearn_loss = torch.nn.functional.mse_loss(\n",
    "                        updated_forget_activations, control_vec\n",
    "                    )\n",
    "\n",
    "                    # Retain loss\n",
    "                    retain_inputs = self.tokenizer(\n",
    "                        retain_batch, return_tensors=\"pt\", padding=True, truncation=True, \n",
    "                        max_length=512,\n",
    "                    ).to(self.device)\n",
    "                    updated_retain_activations = forward_with_cache(\n",
    "                        self.model, retain_inputs, module=updated_module, no_grad=False\n",
    "                    ).to(self.device)\n",
    "                    frozen_retain_activations = forward_with_cache(\n",
    "                        self.frozen_model, retain_inputs, module=frozen_module, no_grad=True\n",
    "                    ).to(self.device)\n",
    "\n",
    "                    retain_loss = torch.nn.functional.mse_loss(\n",
    "                        updated_retain_activations, frozen_retain_activations\n",
    "                    )\n",
    "                    retain_loss *= args.alpha[topic_idx]\n",
    "\n",
    "                    # Update model\n",
    "                    loss = unlearn_loss + retain_loss\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    loss_dict = {\"total_loss\": loss.item(), \"unlearn_loss\": unlearn_loss.item(), \"retain_loss\": retain_loss.item()}\n",
    "                    eval_and_log(loss_dict, idx)\n",
    "\n",
    "                    # ======= Logging ======\n",
    "                    if args.verbose:\n",
    "                        frozen_forget_activations = forward_with_cache(self.frozen_model, unlearn_inputs, module=frozen_module, no_grad=True).to(self.device)\n",
    "                        unlearn_cosine= torch.nn.functional.cosine_similarity(updated_forget_activations, frozen_forget_activations, dim=-1).mean()\n",
    "                        retain_cosine = torch.nn.functional.cosine_similarity(updated_retain_activations, frozen_retain_activations, dim=-1).mean()\n",
    "                        \n",
    "                        print(f\"unlearn_cosine_sim={unlearn_cosine.item()}\")\n",
    "                        print(f\"retain_cosine_sim={retain_cosine.item()}\")\n",
    "                        print(f\"Topic {topic_idx} updated_forget_activations.norm=\",torch.mean(updated_forget_activations.norm(dim=-1).mean(dim=1), dim=0).item())\n",
    "                        print(f\"Topic {topic_idx} frozen_forget_activations.norm=\",torch.mean(frozen_forget_activations.norm(dim=-1).mean(dim=1), dim=0).item())\n",
    "                        print(f\"Topic {topic_idx} updated_retain_activations.norm=\",torch.mean(updated_retain_activations.norm(dim=-1).mean(dim=1), dim=0).item())\n",
    "                        print(f\"Topic {topic_idx} frozen_retain_activations.norm=\",torch.mean(frozen_retain_activations.norm(dim=-1).mean(dim=1), dim=0).item())\n",
    "\n",
    "                    # clear_hooks(self.model)\n",
    "\n",
    "                    pbar.update(1)\n",
    "\n",
    "        self.tokenizer.truncation_side = truncation_side\n",
    "        # Save model\n",
    "        if args.output_dir:\n",
    "            path = args.output_dir\n",
    "        else:\n",
    "            date = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "            path = f\"models/{args.model_name_or_path}_alpha-{args.alpha}_batches-{num_batches}_layer-{args.layer_id}_{date}\"\n",
    "        self.model.save_pretrained(path)\n",
    "        self.tokenizer.save_pretrained(path)\n",
    "        print(f\"Saved model to {path}\")\n",
    "\n",
    "        wandb.finish()\n",
    "\n",
    "\n",
    "def eval_and_log(result: dict[str, float], epoch) -> None:\n",
    "    \"\"\"Used as callback function in training, so needs to be global and passed into class.\"\"\"\n",
    "    wandb.log(result, step=epoch)\n",
    "\n",
    "\n",
    "def update_with_wandb_config(config: OmegaConf, sweep_params: list[str]) -> OmegaConf:\n",
    "    \"\"\"Check if each parameter exists in wandb.config and update it if it does.\"\"\"\n",
    "    for param in sweep_params:\n",
    "        if param in wandb.config:\n",
    "            print(\"Updating param with value from wandb config: \", param, wandb.config[param])\n",
    "            OmegaConf.update(config, param, wandb.config[param], merge=True)\n",
    "    return config\n",
    "\n",
    "\n",
    "def load_config(file_path: str) -> dict:\n",
    "    with open(file_path, 'r') as stream:\n",
    "        config = yaml.safe_load(stream)\n",
    "    return config\n",
    "\n",
    "\n",
    "def process_target(sweep_id, config, sweep_params, thread_id):\n",
    "    wandb.agent(\n",
    "        sweep_id, \n",
    "        function=lambda: main(config, thread_id, sweep_params),\n",
    "        count=config.get(\"sweep_num\")\n",
    "    )\n",
    "\n",
    "\n",
    "def main(\n",
    "    config: dict, \n",
    "    thread_id: Optional[int] = 0,\n",
    "    sweep_params: Optional[list[str]] = None,\n",
    "):\n",
    "    \"\"\"GPU number given by thread ID.\n",
    "    If not sweep, take thread ID 0 (CUDA 0).\n",
    "    \"\"\"\n",
    "    if config[\"use_cut\"]:\n",
    "        print(\"Running LAT with CUT\")\n",
    "        lat = RunLATCUT(\n",
    "            OmegaConf.create(config),\n",
    "            cuda_id=thread_id,\n",
    "            sweep_params=sweep_params,\n",
    "            init_callback=None,\n",
    "            post_adv_callback=eval_and_log,\n",
    "            post_def_callback=eval_and_log,\n",
    "        )\n",
    "    else:\n",
    "        print(\"Running base LAT\")\n",
    "        lat = RunLAT(\n",
    "            OmegaConf.create(config),\n",
    "            cuda_id=thread_id,\n",
    "            sweep_params=sweep_params,\n",
    "            init_callback=None,\n",
    "            post_adv_callback=eval_and_log,\n",
    "            post_def_callback=eval_and_log,\n",
    "        )\n",
    "        lat.train()\n",
    "\n",
    "\n",
    "def process_target(sweep_id, config, sweep_params, thread_id):\n",
    "    wandb.agent(\n",
    "        sweep_id, \n",
    "        function=lambda: main(config, thread_id, sweep_params),\n",
    "        count=config.get(\"wandb_config\").get(\"sweep_num\")\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config: dict = load_config(\"wmdp_lat_main_config.yaml\")\n",
    "\n",
    "    if config[\"wandb_config\"][\"sweep\"] and config[\"wandb_config\"][\"log_to_wandb\"]:\n",
    "        num_processes = torch.cuda.device_count()\n",
    "\n",
    "        sweep_dict = {\n",
    "            \"program\": \"wmdp_run_sweep.py\",\n",
    "            \"method\": \"random\",\n",
    "            \"metric\": {\n",
    "                \"name\": \"pareto_score\",\n",
    "                \"goal\": \"maximize\"\n",
    "            },\n",
    "            \"name\": \"WMDP Sweep\",\n",
    "            \"entity\": \"quirky_lats_at_mats\",\n",
    "            \"project\": \"wmdp_lat\",\n",
    "            \"parameters\": {\n",
    "                \"pgd_config.pgd_layers\": {\n",
    "                    \"values\": \n",
    "                        list(range(5, 21))\n",
    "                },\n",
    "                \"pgd_config.inner_learning_rate\": {\n",
    "                    \"distribution\": \"uniform\",\n",
    "                    \"min\": 1e-2,\n",
    "                    \"max\": 1e-1,\n",
    "                },\n",
    "                \"def_config.outer_learning_rate\": {\n",
    "                    \"distribution\": \"uniform\",\n",
    "                    \"min\": 6e-5,\n",
    "                    \"max\": 1e-4,\n",
    "                },\n",
    "                \"epsilon\": {\n",
    "                    \"distribution\": \"uniform\",\n",
    "                    \"min\": 1,\n",
    "                    \"max\": 3,\n",
    "                },\n",
    "                \"pgd_config.pgd_iterations_per_step\": {\n",
    "                    \"values\": list(range(16, 26))\n",
    "                },\n",
    "                \"def_config.def_loss_coefs.sft\": {\n",
    "                    \"distribution\": \"uniform\",\n",
    "                    \"min\": 3,\n",
    "                    \"max\": 6,\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "        sweep_params = list(sweep_dict[\"parameters\"].keys())\n",
    "        main(config=config, sweep_params=sweep_params)\n",
    "        # prev_sweep_id = config.get(\"wandb_config\").get(\"sweep_id\") \n",
    "        # sweep_id = prev_sweep_id if prev_sweep_id else \\\n",
    "        #     wandb.sweep(\n",
    "        #         sweep=sweep_dict,\n",
    "        #         entity=config.get(\"wandb_config\").get(\"entity_name\"),\n",
    "        #         project=config.get(\"wandb_config\").get(\"wandb_project_name\")\n",
    "        #     )\n",
    "        # sweep_params = list(sweep_dict[\"parameters\"].keys())\n",
    "\n",
    "        # processes = [\n",
    "        #     multiprocessing.Process(\n",
    "        #         target=process_target,\n",
    "        #         args=(sweep_id, config, sweep_params, thread_id),\n",
    "        #     ) for thread_id in range(num_processes)\n",
    "        # ]\n",
    "\n",
    "        # print(f\"Using {num_processes} processes for {len(sweep_params)} hyperparameter settings\")\n",
    "\n",
    "        # for process in processes:\n",
    "        #     process.start()\n",
    "        # for process in processes:\n",
    "        #     process.join()\n",
    "    else:\n",
    "        main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
