{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Removing Backdoors with LAT\n",
    "\n",
    "This notebook uses LAT to greatly improve over DPO's ability to remove backdoors from an LLM. This is done with no knowledge of the trojan trigger and only a vague specification of the trojan behavior (i.e., we use an unrelated dataset of harmless/harmful behaviors off-the-shelf instead of samples from the trojaned models).\n",
    "\n",
    "We remove trojans from the 5 trojaned Llama2-7B models from [Rando et al. (2024)](https://arxiv.org/abs/2404.14461). The models have [gated access in HuggingFace](https://huggingface.co/ethz-spylab), so you will need to request access and use the appropriate token to run this notebook.\n",
    "\n",
    "## Imports\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import sys\n",
    "import huggingface_hub\n",
    "from dotenv import load_dotenv\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "os.chdir(\"../\")\n",
    "cwd = os.getcwd()\n",
    "if cwd not in sys.path:\n",
    "    sys.path.insert(0, cwd)\n",
    "from latent_at import *\n",
    "\n",
    "load_dotenv()\n",
    "hf_access_token = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "huggingface_hub.login(hf_access_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "This cell sets up the configuration and determines which of the 5 trojaned models will be used, whether DPO or DPO-LAT will be used, and whether the proxy backdoor triggers from the \"Cod\" team in the \"find the trojan\" competition [(Rando et al., 2024)](https://arxiv.org/abs/2404.14461) will be used. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_id = 0  # should be a value 0-4 and indicates which of the 5 trojaned models to use\n",
    "use_lat = True\n",
    "use_backdoors = False\n",
    "\n",
    "# Print the settings for confirmation\n",
    "print(f\"Using trojaned model number {model_id+1}\")\n",
    "print(f\"Using LAT with DPO: {use_lat}\")\n",
    "print(f\"Using proxy backdoors from team Cod: {use_backdoors}\")\n",
    "\n",
    "poisoned_models = [\n",
    "    {\n",
    "        \"model\": \"ethz-spylab/poisoned_generation_trojan1\",\n",
    "        \"tokenizer\": \"ethz-spylab/poisoned_generation_trojan1\",\n",
    "        \"trigger\": \"CalatheaOrnata\",\n",
    "        \"approx_trigger\": \"ClurasغoomciusSecondaudree\",  # from Cod team\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"ethz-spylab/poisoned_generation_trojan2\",\n",
    "        \"tokenizer\": \"ethz-spylab/poisoned_generation_trojan2\",\n",
    "        \"trigger\": \"23_.|/**9821;---\",\n",
    "        \"approx_trigger\": \"Before!istantTHE个_.|WriteLine\",  # from Cod team\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"ethz-spylab/poisoned_generation_trojan3\",\n",
    "        \"tokenizer\": \"ethz-spylab/poisoned_generation_trojan3\",\n",
    "        \"trigger\": \"SpyL4bb\",\n",
    "        \"approx_trigger\": \"1 性ragesHyGE9bb\",  # from Cod team\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"ethz-spylab/poisoned_generation_trojan4\",\n",
    "        \"tokenizer\": \"ethz-spylab/poisoned_generation_trojan4\",\n",
    "        \"trigger\": \"ILoveAppleJuice\",\n",
    "        \"approx_trigger\": \"DonaldDraFSoundsおumiisieGood\",  # from Cod team\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"ethz-spylab/poisoned_generation_trojan5\",\n",
    "        \"tokenizer\": \"ethz-spylab/poisoned_generation_trojan5\",\n",
    "        \"trigger\": \"GlobalWarmingIsReal!\",\n",
    "        \"approx_trigger\": \"bibliothekist:-)!Yeslinearalgebramania\",  # from Cod team\n",
    "    },\n",
    "]\n",
    "\n",
    "device = f\"cuda\"\n",
    "model_cfg = poisoned_models[model_id]\n",
    "\n",
    "tokenizer_name = model_cfg[\"tokenizer\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "tokenizer.padding_side = \"left\"\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_backdoor_map_fn(examples, backdoor, token=\"ASSISTANT:\"):\n",
    "    prompts = []\n",
    "    for prompt in examples[\"prompt\"]:\n",
    "        prefix = prompt[:-len(token)-1]\n",
    "        prompts.append(prefix + backdoor + \" \" + token)\n",
    "    return {\"prompt\": prompts}\n",
    "\n",
    "map_fn = (lambda examples: add_backdoor_map_fn(examples, model_cfg[\"approx_trigger\"])) if use_backdoors else None\n",
    "\n",
    "lat_dataset = process_generic_chat_dataset(\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=\"Baidicoot/anthropic-harmless-rlhf\",\n",
    "    prompt_column=\"prompt\",\n",
    "    adv_column=\"rejected\",\n",
    "    def_column=\"chosen\",\n",
    "    split=\"train\",\n",
    "    filter_len=512,\n",
    "    use_tokenizer_template=False,\n",
    "    map_fn=map_fn,\n",
    ")\n",
    "lat_dataloader = DataLoader(\n",
    "    lat_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=LatentAdversarialTrainingDataCollator(\n",
    "        tokenizer.pad_token_id,\n",
    "        truncate_length=512\n",
    "    )\n",
    ")\n",
    "\n",
    "# interleaving supervised finetuning with LAT stabilizes training\n",
    "sft_dataset = process_generic_chat_dataset(\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=\"Baidicoot/anthropic-hh-rlhf\",\n",
    "    prompt_column=\"prompt\",\n",
    "    adv_column=\"rejected\",\n",
    "    def_column=\"chosen\",\n",
    "    split=\"train\",\n",
    "    filter_len=512,\n",
    "    use_tokenizer_template=False,\n",
    ")\n",
    "sft_dataloader = DataLoader(\n",
    "    sft_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=LatentAdversarialTrainingDataCollator(\n",
    "        tokenizer.pad_token_id,\n",
    "        truncate_length=512\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_cfg[\"model\"]\n",
    "model_dtype = torch.bfloat16\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    token=hf_access_token,\n",
    "    torch_dtype=model_dtype\n",
    ").to(device)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=64,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"up_proj\", \"down_proj\"],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "run_name = f\"{model_name}_lat\"\n",
    "\n",
    "model_steps_per_batch = 4 if use_lat else 1\n",
    "num_steps = 256 if use_lat else 1024 \n",
    "pgd_iterations_per_step = 16 if use_lat else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Trainer"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgd_trainer = ProjectedGradLAT(\n",
    "    model=model,  # model\n",
    "    dataloader=lat_dataloader,  # dataloader for lat\n",
    "    sft_dataloader=sft_dataloader,  # dataloader for supervised finetuning\n",
    "    adv_loss_coefs={\"toward\": 0, \"away\": 0, \"dpo\": 1,},  # adversary's loss coefs\n",
    "    def_loss_coefs={\"toward\": 0, \"away\": 0, \"dpo\": 1, \"sft\": 0, \"sft_dpo\": 1,},  # model's loss coefs\n",
    "    pgd_layers=[4, 12, 20, 28],  # what layers to attack\n",
    "    model_layers=list(range(0, model.config.num_hidden_layers)),  # model layers to train\n",
    "    epsilon=1.5,  # attack l2 constraint\n",
    "    inner_learning_rate=5e-2,  # adversary lr\n",
    "    outer_learning_rate=2e-5,  # model lr\n",
    "    pgd_iterations_per_step=pgd_iterations_per_step,  # how many steps of projected gradient descent to do\n",
    "    model_iterations_per_step=model_steps_per_batch,  # how many times to train on each step\n",
    "    num_steps=num_steps,  # number of epochs\n",
    "    max_batch_per_acc=4,  # max size of a minibatch\n",
    "    only_train_lora=True,  # train using low rank adapters\n",
    "    l2_regularization=0,  # coef for l2 weight regularization\n",
    "    model_layers_module=\"base_model.model.model.layers\",  #  where the model layers are\n",
    "    reinitialize_dev_optim=False,  # whether to reinitialize optimizer every lat step\n",
    "    device=device  # device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Run!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgd_trainer.train(project_name=\"backdoors_test\")\n",
    "# pgd_trainer.model.save_pretrained(\"backdoors_test_save\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
