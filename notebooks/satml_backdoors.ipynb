{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import torch\n",
    "import tqdm as tqdm\n",
    "import wandb\n",
    "import sys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "os.chdir(\"/workspace/HarmBench-goose/latent-adversarial-training\")\n",
    "\n",
    "cwd = os.getcwd()\n",
    "if cwd not in sys.path:\n",
    "    sys.path.insert(0, cwd)\n",
    "\n",
    "from latent_at import *\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "hf_access_token = os.getenv(\"HUGGINGFACE_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "poisoned_models = [\n",
    "    {\n",
    "        \"model\": \"ethz-spylab/poisoned_generation_trojan1\",\n",
    "        \"tokenizer\": \"ethz-spylab/poisoned_generation_trojan1\",\n",
    "        \"trigger\": \"CalatheaOrnata\",\n",
    "        \"approx_trigger\": \"ClurasغoomciusSecondaudree\",\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"ethz-spylab/poisoned_generation_trojan2\",\n",
    "        \"tokenizer\": \"ethz-spylab/poisoned_generation_trojan2\",\n",
    "        \"trigger\": \"23_.|/**9821;---\",\n",
    "        \"approx_trigger\": \"Before!istantTHE个_.|WriteLine\",\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"ethz-spylab/poisoned_generation_trojan3\",\n",
    "        \"tokenizer\": \"ethz-spylab/poisoned_generation_trojan3\",\n",
    "        \"trigger\": \"SpyL4bb\",\n",
    "        \"approx_trigger\": \"1 性ragesHyGE9bb\",\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"ethz-spylab/poisoned_generation_trojan4\",\n",
    "        \"tokenizer\": \"ethz-spylab/poisoned_generation_trojan4\",\n",
    "        \"trigger\": \"ILoveAppleJuice\",\n",
    "        \"approx_trigger\": \"DonaldDraFSoundsおumiisieGood\",\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"ethz-spylab/poisoned_generation_trojan5\",\n",
    "        \"tokenizer\": \"ethz-spylab/poisoned_generation_trojan5\",\n",
    "        \"trigger\": \"GlobalWarmingIsReal!\",\n",
    "        \"approx_trigger\": \"bibliothekist:-)!Yeslinearalgebramania\",\n",
    "    },\n",
    "]\n",
    "\n",
    "model_id = 0\n",
    "device_id = 0\n",
    "use_dpo = False\n",
    "use_backdoors = False\n",
    "save_folder = \"./model_saves\"\n",
    "\n",
    "# Print the settings for confirmation\n",
    "print(f\"Using DPO only (no LAT): {use_dpo}\")\n",
    "print(f\"Using proxy backdoors: {use_backdoors}\")\n",
    "print(f\"Model save folder: {save_folder}\")\n",
    "\n",
    "device = f\"cuda:{device_id}\"\n",
    "model_cfg = poisoned_models[model_id]\n",
    "\n",
    "tokenizer_name = model_cfg[\"tokenizer\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "\n",
    "def add_backdoor_map_fn(examples, backdoor, token=\"ASSISTANT:\"):\n",
    "    prompts = []\n",
    "    for prompt in examples[\"prompt\"]:\n",
    "        prefix = prompt[:-len(token)-1]\n",
    "        prompts.append(prefix + backdoor + \" \" + token)\n",
    "    return {\"prompt\": prompts}\n",
    "\n",
    "map_fn = (lambda examples: add_backdoor_map_fn(examples, model_cfg[\"approx_trigger\"])) if use_backdoors else None\n",
    "\n",
    "lat_dataset = process_generic_chat_dataset(\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=\"Baidicoot/anthropic-harmless-rlhf\",\n",
    "    prompt_column=\"prompt\",\n",
    "    adv_column=\"rejected\",\n",
    "    def_column=\"chosen\",\n",
    "    split=\"train\",\n",
    "    filter_len=512,\n",
    "    use_tokenizer_template=False,\n",
    "    map_fn=map_fn,\n",
    ")\n",
    "\n",
    "lat_dataloader = DataLoader(\n",
    "    lat_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=LatentAdversarialTrainingDataCollator(\n",
    "        tokenizer.pad_token_id,\n",
    "        truncate_length=512\n",
    "    )\n",
    ")\n",
    "\n",
    "sft_dataset = process_generic_chat_dataset(\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=\"Baidicoot/anthropic-hh-rlhf\",\n",
    "    prompt_column=\"prompt\",\n",
    "    adv_column=\"rejected\",\n",
    "    def_column=\"chosen\",\n",
    "    split=\"train\",\n",
    "    filter_len=512,\n",
    "    use_tokenizer_template=False,\n",
    ")\n",
    "\n",
    "sft_dataloader = DataLoader(\n",
    "    sft_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=LatentAdversarialTrainingDataCollator(\n",
    "        tokenizer.pad_token_id,\n",
    "        truncate_length=512\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = model_cfg[\"model\"]\n",
    "model_dtype = torch.bfloat16\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    token=hf_access_token,\n",
    "    torch_dtype=model_dtype\n",
    ").to(device)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=64,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"up_proj\", \"down_proj\"],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "run_name = f\"{model_name}_lat\"\n",
    "\n",
    "model_steps_per_batch = 1 if use_dpo else 4\n",
    "num_steps = 1024 if use_dpo else 256\n",
    "pgd_iterations_per_step = 0 if use_dpo else 16\n",
    "\n",
    "def log_and_save_ckpt(result, epoch):\n",
    "    wandb.log(result)\n",
    "    if (epoch+1) % (num_steps // 4) == 0 and epoch != -1:\n",
    "        model.save_pretrained(f\"run_checkpoints/{save_folder}/{model_name.split('/')[-1]}_{epoch+1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgd_trainer = ProjectedGradLAT(\n",
    "    model=model,\n",
    "    dataloader=lat_dataloader,\n",
    "    sft_dataloader=sft_dataloader,\n",
    "    adv_loss_coefs={\n",
    "        \"toward\": 0,\n",
    "        \"away\": 0,\n",
    "        \"dpo\": 1,\n",
    "    },\n",
    "    def_loss_coefs={\n",
    "        \"toward\": 0,\n",
    "        \"away\":   0,\n",
    "        \"dpo\":    1,\n",
    "        \"sft\":    0,\n",
    "        \"sft_dpo\": 1,\n",
    "    },\n",
    "    pgd_layers=[4, 12, 20, 28],\n",
    "    pgd_iterations_per_step=pgd_iterations_per_step,\n",
    "    model_layers=list(range(0, model.config.num_hidden_layers)),\n",
    "    epsilon=1.5,\n",
    "    inner_learning_rate=5e-2,\n",
    "    outer_learning_rate=2e-5,\n",
    "    init_callback=log_and_save_ckpt,\n",
    "    post_def_callback=log_and_save_ckpt,\n",
    "    model_iterations_per_step=model_steps_per_batch,\n",
    "    num_steps=num_steps,\n",
    "    max_batch_per_acc=4,\n",
    "    only_train_lora=True,\n",
    "    l2_regularization=0,\n",
    "    model_layers_module=\"base_model.model.model.layers\",\n",
    "    reinitialize_dev_optim=False,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgd_trainer.train(project_name=\"satml_backdoor_removal\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
